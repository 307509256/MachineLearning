# 方法概论

## 信道（Channel）

深度学习模型的输入数据可以有多个信道。图像就是个典型的例子，它有红、绿和蓝三个颜色信道。一个图像可以被表示成一个三维的张量（Tensor），其中的维度对应于信道、高度和宽度。自然语言数据也可以有多个信道，比如在不同类型的嵌入（embedding）形式中。

## 分批标准化（BN：Batch Normalization）

分批标准化是一种按小批量的方式标准化层输入的技术。它能加速训练过程，允许使用更高的学习率，还可用作规范器（regularizer）。人们发现，分批标准化在卷积和前馈神经网络中应用时非常高效，但尚未被成功应用到循环神经网络上。

论文：分批标准化：通过减少内部协变量位移（Covariate Shift）加速深度网络训练（Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift）
论文：使用分批标准化的循环神经网络（Batch Normalized Recurrent Neural Networks）


## 经验风险最小化
## 正则化/结构风险最小化
降低过拟合风险
### 范数
对$$w$$,正则化参数$$\lambda>0$$，正则项通常用于对模型的训练施加某种约束
- $$L_0$$范数：非零分量的个数
  - 不连续，难以优化求解，因此常使用$$L_1$$范数来近似。
- $$L_1$$范数：绝对值，亦称**LASSO**（最小绝对收缩选择算子）
  - 使被约束矩阵/向量更稀疏。 比$$L_2$$范数更易获得稀疏解，即它求得的$$w$$会有更少的非零分量
- $$L_2$$范数：相当于**模**，亦称**Tikhonov正则化**
  - “岭回归”（ridge regression）
  - 使被约束的矩阵/向量更平滑，因为它对脉冲型的值有很大的惩罚 

